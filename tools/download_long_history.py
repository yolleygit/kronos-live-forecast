#!/usr/bin/env python3
"""
é•¿æœŸå†å²æ•°æ®ä¸‹è½½å™¨
ä»2024å¹´7æœˆå¼€å§‹è·å–å®Œæ•´çš„å†å²æ•°æ®
"""
import sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
import pandas as pd
import logging

# æ·»åŠ æ ¸å¿ƒæ¨¡å—è·¯å¾„
project_root = Path(__file__).parent.parent
core_path = project_root / "core"
sys.path.insert(0, str(core_path))

from ccxt_data_source import CCXTDataSource
from data_manager import DataManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_days_from_july_2024():
    """è®¡ç®—ä»2024å¹´7æœˆ1æ—¥åˆ°ç°åœ¨çš„å¤©æ•°"""
    start_date = datetime(2024, 7, 1, tzinfo=timezone.utc)
    now = datetime.now(timezone.utc)
    
    days_diff = (now - start_date).days
    logger.info(f"ä»2024å¹´7æœˆ1æ—¥åˆ°ç°åœ¨: {days_diff} å¤©")
    return days_diff, start_date

def download_full_history():
    """ä¸‹è½½ä»2024å¹´7æœˆå¼€å§‹çš„å®Œæ•´å†å²æ•°æ®"""
    print("=== é•¿æœŸå†å²æ•°æ®ä¸‹è½½ ===")
    
    # è®¡ç®—éœ€è¦çš„å¤©æ•°
    days_needed, start_date = calculate_days_from_july_2024()
    
    # è½¬æ¢ä¸ºå°æ—¶æ•°ï¼ˆæ¯å¤©24å°æ—¶ï¼‰
    hours_needed = days_needed * 24
    print(f"éœ€è¦ä¸‹è½½çº¦ {hours_needed} å°æ—¶çš„æ•°æ®")
    
    # CCXTé€šå¸¸é™åˆ¶å•æ¬¡è¯·æ±‚çš„æ•°é‡ï¼Œæˆ‘ä»¬éœ€è¦åˆ†æ‰¹è·å–
    max_limit = 1000  # å¤§å¤šæ•°äº¤æ˜“æ‰€çš„é™åˆ¶
    
    if hours_needed <= max_limit:
        print(f"å•æ¬¡è¯·æ±‚å³å¯è·å–æ‰€æœ‰æ•°æ® (éœ€è¦ {hours_needed} å°æ—¶)")
        return download_single_batch(hours_needed)
    else:
        print(f"éœ€è¦åˆ†æ‰¹ä¸‹è½½ (æ€»å…± {hours_needed} å°æ—¶ï¼Œå•æ¬¡æœ€å¤š {max_limit})")
        return download_in_batches(hours_needed, max_limit, start_date)

def download_single_batch(limit):
    """å•æ¬¡ä¸‹è½½æ•°æ®"""
    try:
        ccxt_source = CCXTDataSource()
        
        print("å¼€å§‹ä¸‹è½½å†å²æ•°æ®...")
        df = ccxt_source.fetch_ohlcv_data('BTC/USDT', '1h', limit=limit)
        
        if df is not None:
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {len(df)} æ¡æ•°æ®")
            print(f"æ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            save_to_cache(df)
            return df
        else:
            print("âŒ ä¸‹è½½å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"å•æ¬¡ä¸‹è½½å¤±è´¥: {e}")
        return None

def download_in_batches(total_hours, batch_size, start_date):
    """åˆ†æ‰¹ä¸‹è½½å†å²æ•°æ®"""
    print("âš ï¸ æ³¨æ„: CCXTé€šå¸¸åªèƒ½è·å–æœ€è¿‘1000å°æ—¶çš„æ•°æ®")
    print("å¯¹äºæ›´é•¿çš„å†å²æ•°æ®ï¼Œå¯èƒ½éœ€è¦å…¶ä»–æ•°æ®æºæˆ–API")
    
    # å…ˆå°è¯•è·å–æœ€å¤§å¯èƒ½çš„æ•°æ®
    try:
        ccxt_source = CCXTDataSource()
        
        print(f"å°è¯•è·å–æœ€è¿‘ {batch_size} å°æ—¶çš„æ•°æ®...")
        df = ccxt_source.fetch_ohlcv_data('BTC/USDT', '1h', limit=batch_size)
        
        if df is not None:
            print(f"âœ… è·å–æˆåŠŸ: {len(df)} æ¡æ•°æ®")
            print(f"å®é™…æ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
            
            # æ£€æŸ¥æ˜¯å¦è¦†ç›–åˆ°2024å¹´7æœˆ
            earliest_time = df['timestamps'].min()
            target_start = start_date
            
            if earliest_time >= target_start:
                print(f"âœ… æ•°æ®è¦†ç›–äº†ç›®æ ‡èµ·å§‹æ—¶é—´ {target_start}")
            else:
                print(f"âš ï¸ æ•°æ®æœªå®Œå…¨è¦†ç›–ç›®æ ‡æ—¶é—´èŒƒå›´")
                print(f"   å®é™…å¼€å§‹: {earliest_time}")
                print(f"   ç›®æ ‡å¼€å§‹: {target_start}")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            save_to_cache(df)
            return df
        else:
            print("âŒ è·å–å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"åˆ†æ‰¹ä¸‹è½½å¤±è´¥: {e}")
        return None

def save_to_cache(df):
    """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
    try:
        dm = DataManager()
        
        # å¤‡ä»½ç°æœ‰ç¼“å­˜
        if dm.cache_file.exists():
            backup_file = dm.cache_file.with_suffix('.parquet.backup')
            df_old = pd.read_parquet(dm.cache_file)
            df_old.to_parquet(backup_file)
            logger.info(f"âœ… å·²å¤‡ä»½ç°æœ‰æ•°æ®: {backup_file}")
        
        # ä¿å­˜æ–°æ•°æ®
        df.to_parquet(dm.cache_file)
        dm._save_metadata({"last_update": datetime.now(timezone.utc).isoformat()})
        logger.info(f"âœ… å†å²æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜: {len(df)} æ¡")
        
        # å¯¼å‡ºCSV
        dm._export_to_csv(df)
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        df = download_full_history()
        
        if df is not None:
            print(f"\n=== ä¸‹è½½å®Œæˆ ===")
            print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(df)} æ¡")
            print(f"â° æ—¶é—´è·¨åº¦: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
            print(f"ğŸ’° æœ€æ–°ä»·æ ¼: ${df['close'].iloc[-1]:,.2f}")
            
            # æ£€æŸ¥2024å¹´7æœˆåçš„æ•°æ®
            july_2024 = datetime(2024, 7, 1, tzinfo=timezone.utc)
            data_after_july = df[df['timestamps'] >= july_2024]
            
            print(f"ğŸ“… 2024å¹´7æœˆåæ•°æ®: {len(data_after_july)} æ¡")
            if not data_after_july.empty:
                print(f"   è¦†ç›–æ—¶é—´: {data_after_july['timestamps'].min()} è‡³ {data_after_july['timestamps'].max()}")
        else:
            print("âŒ å†å²æ•°æ®ä¸‹è½½å¤±è´¥")
            
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()