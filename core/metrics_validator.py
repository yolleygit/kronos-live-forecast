"""
12ä¸ªå¢å¼ºæŒ‡æ ‡çš„æ•°å­¦éªŒç®—æ¨¡å—
çº¯æ•°å­¦è¿ç®—éªŒè¯ä»£ç é€»è¾‘æ­£ç¡®æ€§
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, List
import logging


def validate_upside_probabilities(hist_df: pd.DataFrame, close_preds_df: pd.DataFrame, 
                                code_results: Dict, tolerance: float = 0.0001) -> Dict:
    """éªŒç®—ä¸Šæ¶¨æ¦‚ç‡ç›¸å…³æŒ‡æ ‡ï¼ˆ6ä¸ªï¼‰"""
    
    validation_results = {}
    last_close = hist_df['close'].iloc[-1]
    final_hour_preds = close_preds_df.iloc[-1].values
    total_samples = len(final_hour_preds)
    
    # 1. éªŒç®—åŸºç¡€ç›ˆåˆ©æ¦‚ç‡ (upside_0.5%_prob)
    threshold_05 = last_close * 1.005
    manual_count_05 = np.sum(final_hour_preds > threshold_05)
    manual_prob_05 = manual_count_05 / total_samples
    code_prob_05 = code_results.get('upside_0.5%_prob', 0)
    
    validation_results['upside_0.5%_prob'] = {
        'manual_calculation': manual_prob_05,
        'code_result': code_prob_05,
        'difference': abs(manual_prob_05 - code_prob_05),
        'is_valid': abs(manual_prob_05 - code_prob_05) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{manual_count_05}/{total_samples}æ ·æœ¬ > {threshold_05:.2f}"
    }
    
    # 2. éªŒç®—æ˜¾è‘—æ”¶ç›Šæ¦‚ç‡ (upside_2.0%_prob)
    threshold_20 = last_close * 1.02
    manual_count_20 = np.sum(final_hour_preds > threshold_20)
    manual_prob_20 = manual_count_20 / total_samples
    code_prob_20 = code_results.get('upside_2.0%_prob', 0)
    
    validation_results['upside_2.0%_prob'] = {
        'manual_calculation': manual_prob_20,
        'code_result': code_prob_20,
        'difference': abs(manual_prob_20 - code_prob_20),
        'is_valid': abs(manual_prob_20 - code_prob_20) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{manual_count_20}/{total_samples}æ ·æœ¬ > {threshold_20:.2f}"
    }
    
    # 3. éªŒç®—æç«¯æœºä¼šæ¦‚ç‡ (upside_5.0%_prob)
    threshold_50 = last_close * 1.05
    manual_count_50 = np.sum(final_hour_preds > threshold_50)
    manual_prob_50 = manual_count_50 / total_samples
    code_prob_50 = code_results.get('upside_5.0%_prob', 0)
    
    validation_results['upside_5.0%_prob'] = {
        'manual_calculation': manual_prob_50,
        'code_result': code_prob_50,
        'difference': abs(manual_prob_50 - code_prob_50),
        'is_valid': abs(manual_prob_50 - code_prob_50) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{manual_count_50}/{total_samples}æ ·æœ¬ > {threshold_50:.2f}"
    }
    
    # 4. éªŒç®—æœŸæœ›æ”¶ç›Šç‡ (expected_return_%)
    manual_returns = ((final_hour_preds / last_close - 1) * 100)
    manual_expected_return = np.mean(manual_returns)
    code_expected_return = code_results.get('expected_return_%', 0)
    
    validation_results['expected_return_%'] = {
        'manual_calculation': manual_expected_return,
        'code_result': code_expected_return,
        'difference': abs(manual_expected_return - code_expected_return),
        'is_valid': abs(manual_expected_return - code_expected_return) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼šå¹³å‡æ”¶ç›Šç‡ {manual_expected_return:.4f}%"
    }
    
    # 5. éªŒç®—é¢„æµ‹ç½®ä¿¡åº¦ (confidence_score)
    manual_mean = np.mean(final_hour_preds)
    manual_std = np.std(final_hour_preds)
    manual_cv = manual_std / (abs(manual_mean) + 1e-8)
    manual_confidence = 1 / (1 + manual_cv)
    code_confidence = code_results.get('confidence_score', 0)
    
    validation_results['confidence_score'] = {
        'manual_calculation': manual_confidence,
        'code_result': code_confidence,
        'difference': abs(manual_confidence - code_confidence),
        'is_valid': abs(manual_confidence - code_confidence) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼šCV={manual_cv:.4f}, ç½®ä¿¡åº¦={manual_confidence:.4f}"
    }
    
    # 6. éªŒç®—é£é™©è°ƒæ•´æ¦‚ç‡ (risk_adjusted_prob)
    manual_risk_adj = manual_prob_05 * manual_confidence
    code_risk_adj = code_results.get('risk_adjusted_prob', 0)
    
    validation_results['risk_adjusted_prob'] = {
        'manual_calculation': manual_risk_adj,
        'code_result': code_risk_adj,
        'difference': abs(manual_risk_adj - code_risk_adj),
        'is_valid': abs(manual_risk_adj - code_risk_adj) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{manual_prob_05:.4f} Ã— {manual_confidence:.4f} = {manual_risk_adj:.4f}"
    }
    
    return validation_results


def validate_volatility_metrics(hist_df: pd.DataFrame, close_preds_df: pd.DataFrame, 
                               code_results: Dict, config: Dict, tolerance: float = 0.0001) -> Dict:
    """éªŒç®—æ³¢åŠ¨ç‡ç›¸å…³æŒ‡æ ‡ï¼ˆ6ä¸ªï¼‰"""
    
    validation_results = {}
    last_close = hist_df['close'].iloc[-1]
    total_samples = close_preds_df.shape[1]
    
    # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„æ³¢åŠ¨ç‡çª—å£å¤§å°
    forecast_horizon = config.get('data', {}).get('forecast_horizon', 24)
    multiplier = config.get('data', {}).get('volatility_window_multiplier', 1.0)
    volatility_window = int(forecast_horizon * multiplier)
    
    # è®¡ç®—å†å²æ³¢åŠ¨ç‡åŸºå‡†
    hist_log_returns = np.log(hist_df['close'] / hist_df['close'].shift(1)).dropna()
    
    # åŠ¨æ€è®¡ç®—å†å²æ³¢åŠ¨ç‡ï¼ˆä½¿ç”¨é…ç½®çš„çª—å£å¤§å°ï¼‰
    if len(hist_log_returns) >= volatility_window:
        hist_vol_24h = hist_log_returns.iloc[-volatility_window:].std()
    else:
        hist_vol_24h = hist_log_returns.std() if len(hist_log_returns) > 1 else np.nan
    
    # 48å°æ—¶æ³¢åŠ¨ç‡ï¼ˆä½¿ç”¨2å€çª—å£ï¼‰
    window_48h = volatility_window * 2
    if len(hist_log_returns) >= window_48h:
        hist_vol_48h = hist_log_returns.iloc[-window_48h:].std()
    else:
        hist_vol_48h = hist_vol_24h  # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨å•å€ç©—å£
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—é¢„æµ‹æ³¢åŠ¨ç‡
    amplification_factors = []
    vol_24h_amplified_count = 0
    vol_48h_amplified_count = 0
    extreme_vol_count = 0
    
    for col_idx in range(total_samples):
        # æ„å»ºå®Œæ•´ä»·æ ¼åºåˆ—
        pred_series = close_preds_df.iloc[:, col_idx]
        full_sequence = pd.concat([pd.Series([last_close]), pred_series]).reset_index(drop=True)
        
        # è®¡ç®—é¢„æµ‹æ³¢åŠ¨ç‡
        pred_log_returns = np.log(full_sequence / full_sequence.shift(1)).dropna()
        predicted_vol = pred_log_returns.std()
        
        # è®¡ç®—æ”¾å¤§å€æ•°ï¼ˆå¤„ç†NaNå€¼ï¼‰
        if not np.isnan(hist_vol_24h) and hist_vol_24h > 1e-8:
            amplification_factor = predicted_vol / hist_vol_24h
        else:
            amplification_factor = np.nan
        amplification_factors.append(amplification_factor)
        
        # æ³¢åŠ¨ç‡åŸºå‡†æ¯”è¾ƒï¼ˆå¤„ç†NaNå€¼ï¼‰
        if not np.isnan(hist_vol_24h) and predicted_vol > hist_vol_24h:
            vol_24h_amplified_count += 1
            
        # 48å°æ—¶åŸºå‡†æ¯”è¾ƒ
        if not np.isnan(hist_vol_48h) and predicted_vol > hist_vol_48h:
            vol_48h_amplified_count += 1
            
        # æç«¯æ³¢åŠ¨æ£€æŸ¥ (2å€é˜ˆå€¼)
        if not np.isnan(amplification_factor) and amplification_factor > 2.0:
            extreme_vol_count += 1
    
    # 1. éªŒç®—24å°æ—¶æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡
    manual_vol_24h_prob = vol_24h_amplified_count / total_samples
    code_vol_24h_prob = code_results.get('vol_amp_prob_24h', 0)
    
    # å¤„ç†æ³¢åŠ¨ç‡æ˜¾ç¤º
    vol_24h_display = f"{hist_vol_24h:.6f}" if not np.isnan(hist_vol_24h) else "NaN"
    
    validation_results['vol_amp_prob_24h'] = {
        'manual_calculation': manual_vol_24h_prob,
        'code_result': code_vol_24h_prob,
        'difference': abs(manual_vol_24h_prob - code_vol_24h_prob),
        'is_valid': abs(manual_vol_24h_prob - code_vol_24h_prob) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{vol_24h_amplified_count}/{total_samples}æ ·æœ¬è¶…è¿‡{volatility_window}håŸºå‡†æ³¢åŠ¨ç‡{vol_24h_display}"
    }
    
    # 2. éªŒç®—48å°æ—¶æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡
    manual_vol_48h_prob = vol_48h_amplified_count / total_samples
    code_vol_48h_prob = code_results.get('vol_amp_prob_48h', 0)
    
    # å¤„ç†48hæ³¢åŠ¨ç‡æ˜¾ç¤º
    vol_48h_display = f"{hist_vol_48h:.6f}" if not np.isnan(hist_vol_48h) else "NaN"
    
    validation_results['vol_amp_prob_48h'] = {
        'manual_calculation': manual_vol_48h_prob,
        'code_result': code_vol_48h_prob,
        'difference': abs(manual_vol_48h_prob - code_vol_48h_prob),
        'is_valid': abs(manual_vol_48h_prob - code_vol_48h_prob) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{vol_48h_amplified_count}/{total_samples}æ ·æœ¬è¶…è¿‡{window_48h}håŸºå‡†æ³¢åŠ¨ç‡{vol_48h_display}"
    }
    
    # 3. éªŒç®—å¹³å‡æ”¾å¤§å€æ•°
    manual_avg_amplification = np.mean(amplification_factors)
    code_avg_amplification = code_results.get('avg_amplification_factor', 0)
    
    # å¤„ç†NaNçš„æƒ…å†µ
    if np.isnan(manual_avg_amplification) and np.isnan(code_avg_amplification):
        is_valid_amp = True
        diff_amp = 0
    elif np.isnan(manual_avg_amplification) or np.isnan(code_avg_amplification):
        is_valid_amp = False
        diff_amp = float('inf')
    else:
        diff_amp = abs(manual_avg_amplification - code_avg_amplification)
        is_valid_amp = diff_amp < tolerance
    
    validation_results['avg_amplification_factor'] = {
        'manual_calculation': manual_avg_amplification,
        'code_result': code_avg_amplification,
        'difference': diff_amp,
        'is_valid': is_valid_amp,
        'details': f"æ‰‹åŠ¨ï¼šå¹³å‡æ”¾å¤§å€æ•° {manual_avg_amplification:.4f}"
    }
    
    # 4. éªŒç®—æç«¯æ³¢åŠ¨æ¦‚ç‡
    manual_extreme_prob = extreme_vol_count / total_samples
    code_extreme_prob = code_results.get('extreme_vol_prob', 0)
    
    validation_results['extreme_vol_prob'] = {
        'manual_calculation': manual_extreme_prob,
        'code_result': code_extreme_prob,
        'difference': abs(manual_extreme_prob - code_extreme_prob),
        'is_valid': abs(manual_extreme_prob - code_extreme_prob) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{extreme_vol_count}/{total_samples}æ ·æœ¬æ”¾å¤§å€æ•°>2.0"
    }
    
    # 5. æ³¢åŠ¨æŒç»­æ€§è¯„åˆ† (å¤æ‚è®¡ç®—ï¼Œæš‚æ—¶è·³è¿‡è¯¦ç»†éªŒç®—)
    code_persistence_score = code_results.get('vol_persistence_score', 0)
    validation_results['vol_persistence_score'] = {
        'manual_calculation': 'skipped',
        'code_result': code_persistence_score,
        'difference': 'skipped',
        'is_valid': True,  # æš‚æ—¶æ ‡è®°ä¸ºé€šè¿‡
        'details': "å¤æ‚ç®—æ³•ï¼Œæš‚æ—¶è·³è¿‡æ‰‹åŠ¨éªŒç®—"
    }
    
    # 6. éªŒç®—ç»¼åˆæ³¢åŠ¨é£é™©è¯„åˆ†
    # æ ¹æ®ä»£ç é€»è¾‘æ‰‹åŠ¨è®¡ç®—
    weights = {
        'vol_amp_prob_24h': 0.25,
        'avg_amplification_factor': 0.20, 
        'extreme_vol_prob': 0.20,
        'vol_persistence_score': 0.15,
        'vol_amp_prob_48h': 0.20
    }
    
    # æ ‡å‡†åŒ–å„é¡¹æŒ‡æ ‡
    norm_vol_24h = min(100, manual_vol_24h_prob * 100)
    norm_vol_48h = min(100, manual_vol_48h_prob * 100)
    if not np.isnan(manual_avg_amplification):
        norm_avg_amp = min(100, (manual_avg_amplification / 2.0) * 100)
    else:
        norm_avg_amp = 0
    norm_extreme = min(100, (manual_extreme_prob / 0.3) * 100)
    norm_persistence = code_persistence_score * 100  # ä½¿ç”¨ä»£ç ç»“æœ
    
    manual_overall_score = (
        norm_vol_24h * weights['vol_amp_prob_24h'] +
        norm_vol_48h * weights['vol_amp_prob_48h'] +
        norm_avg_amp * weights['avg_amplification_factor'] +
        norm_extreme * weights['extreme_vol_prob'] +
        norm_persistence * weights['vol_persistence_score']
    )
    manual_overall_score = round(manual_overall_score, 0)
    code_overall_score = code_results.get('overall_vol_risk_score', 0)
    
    validation_results['overall_vol_risk_score'] = {
        'manual_calculation': manual_overall_score,
        'code_result': code_overall_score,
        'difference': abs(manual_overall_score - code_overall_score),
        'is_valid': abs(manual_overall_score - code_overall_score) < 1,  # å…è®¸1åˆ†è¯¯å·®
        'details': f"æ‰‹åŠ¨ï¼šåŠ æƒè®¡ç®— {manual_overall_score}/100"
    }
    
    return validation_results


def validate_traditional_metrics(hist_df: pd.DataFrame, close_preds_df: pd.DataFrame,
                               code_results: Dict, tolerance: float = 0.0001) -> Dict:
    """éªŒç®—ä¼ ç»ŸæŒ‡æ ‡ï¼ˆ2ä¸ªï¼‰"""
    
    validation_results = {}
    last_close = hist_df['close'].iloc[-1]
    final_hour_preds = close_preds_df.iloc[-1].values
    total_samples = len(final_hour_preds)
    
    # 1. éªŒç®—ä¼ ç»Ÿä¸Šæ¶¨æ¦‚ç‡
    manual_upside_count = np.sum(final_hour_preds > last_close)
    manual_traditional_upside = manual_upside_count / total_samples
    code_traditional_upside = code_results.get('traditional_upside_prob', 0)
    
    validation_results['traditional_upside_prob'] = {
        'manual_calculation': manual_traditional_upside,
        'code_result': code_traditional_upside,
        'difference': abs(manual_traditional_upside - code_traditional_upside),
        'is_valid': abs(manual_traditional_upside - code_traditional_upside) < tolerance,
        'details': f"æ‰‹åŠ¨ï¼š{manual_upside_count}/{total_samples}æ ·æœ¬ > {last_close:.2f}"
    }
    
    # 2. éªŒç®—ä¼ ç»Ÿæ³¢åŠ¨æ”¾å¤§æ¦‚ç‡ (ä¸vol_amp_prob_24hç›¸åŒ)
    code_traditional_vol = code_results.get('traditional_vol_amp_prob', 0)
    code_vol_24h = code_results.get('vol_amp_prob_24h', 0)
    
    validation_results['traditional_vol_amp_prob'] = {
        'manual_calculation': code_vol_24h,  # åº”è¯¥ä¸vol_amp_prob_24hç›¸åŒ
        'code_result': code_traditional_vol,
        'difference': abs(code_vol_24h - code_traditional_vol),
        'is_valid': abs(code_vol_24h - code_traditional_vol) < tolerance,
        'details': f"åº”è¯¥ä¸vol_amp_prob_24hç›¸åŒ: {code_vol_24h:.4f}"
    }
    
    return validation_results


def get_adaptive_tolerance(metric_name: str, value: float, config: Dict) -> float:
    """è·å–è‡ªé€‚åº”å®¹å·®"""
    
    validation_config = config.get('validation', {})
    
    # æ£€æŸ¥ç‰¹å®šæŒ‡æ ‡å®¹å·®è¦†ç›–
    specific_tolerance = validation_config.get('metric_specific_tolerance', {})
    if metric_name in specific_tolerance:
        return specific_tolerance[metric_name]
    
    # æŒ‰ç±»å‹ç¡®å®šå®¹å·®
    tolerance_by_type = validation_config.get('tolerance_by_type', {})
    
    if 'prob' in metric_name or 'confidence' in metric_name:
        return tolerance_by_type.get('probability', 0.0001)
    elif 'score' in metric_name:
        return tolerance_by_type.get('score', 1.0)
    elif 'factor' in metric_name or 'amplification' in metric_name:
        return tolerance_by_type.get('ratio', 0.001)
    elif '%' in metric_name or 'return' in metric_name:
        return tolerance_by_type.get('percentage', 0.01)
    else:
        return validation_config.get('default_tolerance', 0.0001)


def save_validation_results(validation_report: Dict, config: Dict) -> str:
    """ä¿å­˜éªŒç®—ç»“æœåˆ°æ–‡ä»¶"""
    import json
    from pathlib import Path
    from datetime import datetime, timezone
    
    # åˆ›å»ºéªŒç®—ç»“æœç›®å½•
    validation_dir = Path("validation_results")
    validation_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
    timestamp = datetime.now(timezone.utc)
    timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        "timestamp": timestamp.isoformat(),
        "config_snapshot": {
            "forecast_horizon": config.get('data', {}).get('forecast_horizon', 1),
            "volatility_window": int(config.get('data', {}).get('forecast_horizon', 24) * config.get('data', {}).get('volatility_window_multiplier', 1.0)),
            "num_samples": config.get('sampling', {}).get('num_samples', 30),
            "validation_enabled": config.get('validation', {}).get('enable_metrics_validation', True)
        },
        "validation_report": validation_report
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_file = validation_dir / f"validation_{timestamp_str}.json"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜æœ€æ–°ç»“æœï¼ˆè¦†ç›–ï¼‰
    latest_file = validation_dir / "latest_validation.json"
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
    
    # æ›´æ–°éªŒç®—å†å²è®°å½•
    update_validation_history(validation_report, config)
    
    return str(detailed_file)


def update_validation_history(validation_report: Dict, config: Dict, max_history: int = 100):
    """æ›´æ–°éªŒç®—å†å²è®°å½•"""
    import json
    from pathlib import Path
    from datetime import datetime, timezone
    
    history_file = Path("validation_results") / "validation_history.json"
    
    # è¯»å–ç°æœ‰å†å²
    if history_file.exists():
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            history = {"records": []}
    else:
        history = {"records": []}
    
    # æ·»åŠ æ–°è®°å½•
    new_record = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "summary": validation_report.get('summary', {}),
        "config": {
            "forecast_horizon": config.get('data', {}).get('forecast_horizon', 1),
            "volatility_window": int(config.get('data', {}).get('forecast_horizon', 24) * config.get('data', {}).get('volatility_window_multiplier', 1.0)),
            "num_samples": config.get('sampling', {}).get('num_samples', 30)
        }
    }
    
    history["records"].append(new_record)
    
    # ä¿æŒæœ€è¿‘max_historyæ¡è®°å½•
    if len(history["records"]) > max_history:
        history["records"] = history["records"][-max_history:]
    
    # ä¿å­˜æ›´æ–°åçš„å†å²
    history_file.parent.mkdir(exist_ok=True)
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ“ éªŒç®—å†å²å·²æ›´æ–°: {len(history['records'])}æ¡è®°å½•")


def validate_all_metrics(hist_df: pd.DataFrame, close_preds_df: pd.DataFrame,
                        enhanced_metrics: Dict, config: Dict) -> Dict:
    """éªŒç®—æ‰€æœ‰12ä¸ªå¢å¼ºæŒ‡æ ‡ + 2ä¸ªä¼ ç»ŸæŒ‡æ ‡"""
    
    tolerance = config.get('validation', {}).get('validation_tolerance', 0.0001)
    show_details = config.get('validation', {}).get('show_validation_details', False)
    
    # æ‰§è¡Œå„ç±»æŒ‡æ ‡éªŒç®—
    upside_validation = validate_upside_probabilities(hist_df, close_preds_df, enhanced_metrics, tolerance)
    volatility_validation = validate_volatility_metrics(hist_df, close_preds_df, enhanced_metrics, config, tolerance)
    traditional_validation = validate_traditional_metrics(hist_df, close_preds_df, enhanced_metrics, tolerance)
    
    # åˆå¹¶æ‰€æœ‰éªŒç®—ç»“æœ
    all_validation = {
        **upside_validation,
        **volatility_validation, 
        **traditional_validation
    }
    
    # ç»Ÿè®¡éªŒç®—ç»“æœ
    total_metrics = len(all_validation)
    passed_metrics = sum(1 for v in all_validation.values() if v['is_valid'])
    failed_metrics = total_metrics - passed_metrics
    
    # ç”ŸæˆéªŒç®—æŠ¥å‘Š
    validation_report = {
        'summary': {
            'total_metrics': total_metrics,
            'passed': passed_metrics,
            'failed': failed_metrics,
            'pass_rate': f"{(passed_metrics/total_metrics)*100:.1f}%"
        },
        'details': all_validation
    }
    
    # ä¿å­˜éªŒç®—ç»“æœåˆ°æ–‡ä»¶
    try:
        saved_file = save_validation_results(validation_report, config)
        validation_report['saved_file'] = saved_file
        print(f"âœ“ éªŒç®—ç»“æœå·²ä¿å­˜åˆ°: {saved_file}")
    except Exception as e:
        print(f"âš  ä¿å­˜éªŒç®—ç»“æœå¤±è´¥: {e}")
    
    # æ‰“å°éªŒç®—ç»“æœ
    if show_details:
        print(f"\nğŸ” å¢å¼ºæŒ‡æ ‡æ•°å­¦éªŒç®—æŠ¥å‘Š")
        print(f"{'='*50}")
        print(f"æ€»æŒ‡æ ‡æ•°: {total_metrics}")
        print(f"éªŒç®—é€šè¿‡: {passed_metrics}")
        print(f"éªŒç®—å¤±è´¥: {failed_metrics}")
        print(f"é€šè¿‡ç‡: {validation_report['summary']['pass_rate']}")
        
        if failed_metrics > 0:
            print(f"\nâŒ éªŒç®—å¤±è´¥çš„æŒ‡æ ‡:")
            for metric_name, result in all_validation.items():
                if not result['is_valid']:
                    print(f"  â€¢ {metric_name}: å·®å¼‚ {result['difference']:.6f}")
                    print(f"    ä»£ç ç»“æœ: {result['code_result']}")
                    print(f"    æ‰‹åŠ¨è®¡ç®—: {result['manual_calculation']}")
                    print(f"    è¯¦æƒ…: {result['details']}")
    else:
        # ç®€åŒ–è¾“å‡º
        if failed_metrics == 0:
            print(f"âœ… æŒ‡æ ‡éªŒç®—: å…¨éƒ¨{total_metrics}ä¸ªæŒ‡æ ‡é€šè¿‡éªŒç®— (å®¹å·®Â±{tolerance})")
        else:
            print(f"âš ï¸ æŒ‡æ ‡éªŒç®—: {passed_metrics}/{total_metrics}æŒ‡æ ‡é€šè¿‡ï¼Œ{failed_metrics}ä¸ªå¤±è´¥")
    
    return validation_report


def should_stop_on_validation_error(validation_report: Dict, config: Dict) -> bool:
    """æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å› éªŒç®—é”™è¯¯åœæ­¢è¿è¡Œ"""
    
    stop_on_error = config.get('validation', {}).get('stop_on_validation_error', False)
    failed_count = validation_report['summary']['failed']
    
    if stop_on_error and failed_count > 0:
        print(f"ğŸ›‘ é…ç½®è¦æ±‚éªŒç®—é”™è¯¯æ—¶åœæ­¢è¿è¡Œï¼Œæ£€æµ‹åˆ°{failed_count}ä¸ªæŒ‡æ ‡éªŒç®—å¤±è´¥")
        return True
    
    return False