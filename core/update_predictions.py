import gc
import os
import re
import subprocess
import time
import yaml
import logging
import random
from datetime import datetime, timezone, timedelta
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from binance.client import Client

from model import KronosTokenizer, Kronos, KronosPredictor
from enhanced_metrics import calculate_all_enhanced_metrics, format_metrics_for_display


def setup_random_seeds(config):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random_seed = config.get('sampling', {}).get('random_seed', 42)
    enable_deterministic = config.get('sampling', {}).get('enable_deterministic', True)
    
    if enable_deterministic:
        # è®¾ç½®Pythonæ ‡å‡†åº“éšæœºç§å­
        random.seed(random_seed)
        
        # è®¾ç½®NumPyéšæœºç§å­
        np.random.seed(random_seed)
        
        # è®¾ç½®PyTorchéšæœºç§å­
        torch.manual_seed(random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(random_seed)
            torch.cuda.manual_seed_all(random_seed)
        
        # è®¾ç½®PyTorchç¡®å®šæ€§æ¨¡å¼
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        print(f"âœ… éšæœºç§å­å·²è®¾ç½®ä¸º {random_seed}ï¼Œå¯ç”¨ç¡®å®šæ€§æ¨¡å¼")
    else:
        print(f"âš ï¸ ç¡®å®šæ€§æ¨¡å¼å·²ç¦ç”¨ï¼Œç»“æœå¯èƒ½ä¸å¯é‡ç°")


def load_config(config_path=None):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        # æ ¹æ®å½“å‰è„šæœ¬ä½ç½®åŠ¨æ€ç¡®å®šé…ç½®è·¯å¾„
        current_dir = Path(__file__).parent
        config_path = current_dir.parent / "configs" / "config.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as file:
            config = yaml.safe_load(file)
        print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return get_default_config()
    except yaml.YAMLError as e:
        print(f"é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return get_default_config()


def get_default_config():
    """è·å–é»˜è®¤é…ç½®"""
    return {
        "model": {
            "model_name": "Kronos-small",
            "tokenizer_name": "Kronos-Tokenizer-base", 
            "model_path": "../Kronos_model",
            "max_context": 512,
            "device": "cpu"
        },
        "sampling": {
            "temperature": 0.6,
            "top_p": 0.9,
            "num_samples": 30,
            "volatility_temperature_multiplier": 1.0
        },
        "data": {
            "symbol": "BTCUSDT",
            "timeframe": "1h", 
            "history_window": 360,
            "forecast_horizon": 24,
            "volatility_window_multiplier": 1.0
        },
        "api": {
            "binance_base_url": "https://api.binance.com",
            "request_timeout": 30,
            "retry_attempts": 3
        },
        "output": {
            "chart_filename": "frontend/prediction_chart.png",
            "html_filename": "frontend/index.html", 
            "auto_commit": True,
            "auto_push": False,
            "web_server_port": 8000
        },
        "logging": {
            "level": "INFO",
            "enable_file_logging": False
        }
    }


# åŠ è½½é…ç½®
CONFIG = load_config()

# å‘åå…¼å®¹çš„é…ç½®æ˜ å°„
Config = {
    "REPO_PATH": Path(__file__).parent.parent.resolve(),  # ä¿®å¤ï¼šæŒ‡å‘é¡¹ç›®æ ¹ç›®å½•
    "MODEL_PATH": CONFIG["model"]["model_path"],
    "MODEL_NAME": CONFIG["model"]["model_name"],
    "TOKENIZER_NAME": CONFIG["model"]["tokenizer_name"],
    "MAX_CONTEXT": CONFIG["model"]["max_context"],
    "DEVICE": CONFIG["model"]["device"],
    "TEMPERATURE": CONFIG["sampling"]["temperature"],
    "TOP_P": CONFIG["sampling"]["top_p"],
    "SYMBOL": CONFIG["data"]["symbol"],
    "INTERVAL": CONFIG["data"]["timeframe"],
    "HIST_POINTS": CONFIG["data"]["history_window"],
    "PRED_HORIZON": CONFIG["data"]["forecast_horizon"],
    "N_PREDICTIONS": CONFIG["sampling"]["num_samples"],
    "VOL_WINDOW": int(CONFIG["data"]["forecast_horizon"] * CONFIG["data"]["volatility_window_multiplier"]),
    "VOL_TEMP_MULTIPLIER": CONFIG["sampling"]["volatility_temperature_multiplier"],
    "AUTO_COMMIT": CONFIG["output"]["auto_commit"],
    "AUTO_PUSH": CONFIG["output"]["auto_push"],
}

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, CONFIG["logging"]["level"]),
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model_configs():
    """åŠ è½½æ¨¡å‹é…ç½®å‚æ•°"""
    tokenizer_name = Config["TOKENIZER_NAME"]
    model_name = Config["MODEL_NAME"]
    
    # åŠ è½½tokenizeré…ç½® 
    tokenizer_config_path = f"{Config['MODEL_PATH']}/{tokenizer_name}/config.json"
    model_config_path = f"{Config['MODEL_PATH']}/{model_name}/config.json"
    
    try:
        import json
        with open(tokenizer_config_path, 'r') as f:
            tokenizer_config = json.load(f)
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        
        logger.info(f"åŠ è½½tokenizeré…ç½®: {tokenizer_name}")
        logger.info(f"åŠ è½½æ¨¡å‹é…ç½®: {model_name}")
        
        return tokenizer_config, model_config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise


def load_model():
    """Loads the Kronos model and tokenizer with dynamic configuration."""
    logger.info(f"Loading Kronos model: {Config['MODEL_NAME']}...")
    
    # åŠ¨æ€åŠ è½½é…ç½®
    tokenizer_config, model_config = load_model_configs()
    
    # åˆ›å»ºtokenizerå’Œmodelå®ä¾‹
    tokenizer = KronosTokenizer(
        d_in=6,  # å›ºå®šè¾“å…¥ç»´åº¦ (OHLCVA)
        d_model=tokenizer_config['d_model'],
        n_heads=tokenizer_config['n_heads'],
        ff_dim=tokenizer_config['ff_dim'],
        n_enc_layers=tokenizer_config['n_enc_layers'],
        n_dec_layers=tokenizer_config['n_dec_layers'],
        ffn_dropout_p=tokenizer_config['ffn_dropout_p'],
        attn_dropout_p=tokenizer_config['attn_dropout_p'],
        resid_dropout_p=tokenizer_config['resid_dropout_p'],
        s1_bits=tokenizer_config['s1_bits'],
        s2_bits=tokenizer_config['s2_bits'],
        beta=tokenizer_config['beta'],
        gamma0=tokenizer_config['gamma0'],
        gamma=tokenizer_config['gamma'],
        zeta=tokenizer_config['zeta'],
        group_size=tokenizer_config['group_size']
    )
    
    model = Kronos(
        d_model=model_config['d_model'],
        n_heads=model_config['n_heads'],
        ff_dim=model_config['ff_dim'],
        n_layers=model_config['n_layers'],
        ffn_dropout_p=model_config['ffn_dropout_p'],
        attn_dropout_p=model_config['attn_dropout_p'],
        resid_dropout_p=model_config['resid_dropout_p'],
        s1_bits=model_config['s1_bits'],
        s2_bits=model_config['s2_bits'],
        token_dropout_p=model_config['token_dropout_p'],
        learn_te=model_config['learn_te']
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    tokenizer_path = f"{Config['MODEL_PATH']}/{Config['TOKENIZER_NAME']}"
    model_path = f"{Config['MODEL_PATH']}/{Config['MODEL_NAME']}"
    
    from safetensors.torch import load_file
    tokenizer.load_state_dict(load_file(f"{tokenizer_path}/model.safetensors"))
    model.load_state_dict(load_file(f"{model_path}/model.safetensors"))
    
    tokenizer.eval()
    model.eval()
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = KronosPredictor(
        model, 
        tokenizer, 
        device=Config["DEVICE"], 
        max_context=Config["MAX_CONTEXT"]
    )
    
    # è·å–æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {Config['MODEL_NAME']}")
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    logger.info(f"æ¨ç†è®¾å¤‡: {Config['DEVICE']}")
    
    return predictor


def make_prediction(df, predictor):
    """Generates probabilistic forecasts using the Kronos model."""
    last_timestamp = df['timestamps'].max()
    start_new_range = last_timestamp + pd.Timedelta(hours=1)
    new_timestamps_index = pd.date_range(
        start=start_new_range,
        periods=Config["PRED_HORIZON"],
        freq='h'
    )
    y_timestamp = pd.Series(new_timestamps_index, name='y_timestamp')
    x_timestamp = df['timestamps']
    x_df = df[['open', 'high', 'low', 'close', 'volume', 'amount']]

    vol_temp_multiplier = Config["VOL_TEMP_MULTIPLIER"]
    
    with torch.no_grad():
        # å¦‚æœæ³¢åŠ¨æ€§æ¸©åº¦å€æ•°ä¸º1.0ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡é¢„æµ‹
        if vol_temp_multiplier == 1.0:
            logger.info(f"å¼€å§‹å•æ¬¡é¢„æµ‹ (T={Config['TEMPERATURE']}, top_p={Config['TOP_P']}, samples={Config['N_PREDICTIONS']})...")
            begin_time = time.time()
            close_preds_main, volume_preds_main = predictor.predict(
                df=x_df, x_timestamp=x_timestamp, y_timestamp=y_timestamp,
                pred_len=Config["PRED_HORIZON"], T=Config["TEMPERATURE"], 
                top_p=Config["TOP_P"], sample_count=Config["N_PREDICTIONS"], 
                verbose=True
            )
            elapsed_time = time.time() - begin_time
            logger.info(f"å•æ¬¡é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            
            # æ³¢åŠ¨æ€§é¢„æµ‹ä½¿ç”¨ç›¸åŒçš„ç»“æœ
            close_preds_volatility = close_preds_main
            logger.info(f"ä½¿ç”¨ç›¸åŒé¢„æµ‹ç»“æœè¿›è¡Œæ³¢åŠ¨æ€§åˆ†æ (æ¸©åº¦å€æ•°={vol_temp_multiplier})")
            
        else:
            # å¦‚æœæ³¢åŠ¨æ€§æ¸©åº¦å€æ•°ä¸ä¸º1.0ï¼Œåˆ™è¿›è¡Œä¸¤æ¬¡é¢„æµ‹
            vol_temperature = Config["TEMPERATURE"] * vol_temp_multiplier
            
            logger.info(f"å¼€å§‹ä¸»è¦é¢„æµ‹ (T={Config['TEMPERATURE']}, top_p={Config['TOP_P']}, samples={Config['N_PREDICTIONS']})...")
            begin_time = time.time()
            close_preds_main, volume_preds_main = predictor.predict(
                df=x_df, x_timestamp=x_timestamp, y_timestamp=y_timestamp,
                pred_len=Config["PRED_HORIZON"], T=Config["TEMPERATURE"], 
                top_p=Config["TOP_P"], sample_count=Config["N_PREDICTIONS"], 
                verbose=True
            )
            elapsed_time = time.time() - begin_time
            logger.info(f"ä¸»è¦é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

            # ç”Ÿæˆæ³¢åŠ¨æ€§é¢„æµ‹ (ä½¿ç”¨é…ç½®çš„æ¸©åº¦å€æ•°)
            logger.info(f"å¼€å§‹æ³¢åŠ¨æ€§é¢„æµ‹ (T={vol_temperature:.2f}, top_p={Config['TOP_P']}, å€æ•°={vol_temp_multiplier})...")
            begin_time = time.time()
            close_preds_volatility, _ = predictor.predict(
                df=x_df, x_timestamp=x_timestamp, y_timestamp=y_timestamp,
                pred_len=Config["PRED_HORIZON"], T=vol_temperature, 
                top_p=Config["TOP_P"], sample_count=Config["N_PREDICTIONS"], 
                verbose=True
            )
            elapsed_time = time.time() - begin_time
            logger.info(f"æ³¢åŠ¨æ€§é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

    return close_preds_main, volume_preds_main, close_preds_volatility


def fetch_binance_data():
    """è·å–å¸‚åœºæ•°æ® - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
    if CONFIG.get("data", {}).get("cache_enabled", False):
        try:
            from data_manager import data_manager
            logger.info("ä½¿ç”¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨è·å–æ•°æ®...")
            
            # è·å–ç¼“å­˜çŠ¶æ€
            status = data_manager.get_cache_status()
            logger.info(f"ç¼“å­˜çŠ¶æ€: {status['update_reason']}")
            
            # è·å–æ•°æ®
            df = data_manager.get_data()
            if df is not None:
                logger.info(f"ä»ç¼“å­˜è·å–{len(df)}æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
                return df
            else:
                logger.warning("ç¼“å­˜è·å–å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥APIè·å–")
                
        except Exception as e:
            logger.error(f"æ•°æ®ç®¡ç†å™¨é”™è¯¯: {e}")
            logger.warning("å›é€€åˆ°ä¼ ç»Ÿè·å–æ–¹å¼")
    
    # ä¼ ç»Ÿè·å–æ–¹å¼ (åå¤‡æ–¹æ¡ˆ)
    return fetch_binance_data_legacy()


def fetch_binance_data_legacy():
    """ä¼ ç»Ÿçš„æ•°æ®è·å–æ–¹å¼ (åå¤‡æ–¹æ¡ˆ)"""
    symbol, interval = Config["SYMBOL"], Config["INTERVAL"]
    limit = Config["HIST_POINTS"] + Config["VOL_WINDOW"]

    logger.info(f"ä»äº¤æ˜“æ‰€è·å–{limit}æ¡ {symbol} {interval} æ•°æ®...")
    
    try:
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨ binance åº“
        client = Client()
        klines = client.get_klines(symbol=symbol, interval=interval, limit=limit)
        logger.info("é€šè¿‡ binance åº“è·å–æ•°æ®æˆåŠŸ")
    except Exception as e1:
        logger.warning(f"æ–¹æ³•1å¤±è´¥: {e1}")
        try:
            # æ–¹æ³•2: ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨API
            import requests
            url = f"https://api.binance.com/api/v3/klines"
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': limit
            }
            
            session = requests.Session()
            # è®¾ç½®è¶…æ—¶å’Œé‡è¯•
            response = session.get(url, params=params, timeout=30)
            response.raise_for_status()
            klines = response.json()
            logger.info("é€šè¿‡ç›´æ¥APIè°ƒç”¨è·å–æ•°æ®æˆåŠŸ")
        except Exception as e2:
            logger.warning(f"æ–¹æ³•2ä¹Ÿå¤±è´¥: {e2}")
            # æ–¹æ³•3: ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
            return generate_mock_data(limit)

    cols = ['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time',
            'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume',
            'taker_buy_quote_asset_volume', 'ignore']
    df = pd.DataFrame(klines, columns=cols)

    df = df[['open_time', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']]
    df.rename(columns={'quote_asset_volume': 'amount', 'open_time': 'timestamps'}, inplace=True)

    df['timestamps'] = pd.to_datetime(df['timestamps'], unit='ms', utc=True)
    for col in ['open', 'high', 'low', 'close', 'volume', 'amount']:
        df[col] = pd.to_numeric(df[col])

    logger.info("ä¼ ç»Ÿæ–¹å¼è·å–æ•°æ®æˆåŠŸ")
    return df


def generate_mock_data(limit):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„BTCä»·æ ¼æ•°æ®ç”¨äºæµ‹è¯•"""
    import random
    from datetime import timedelta
    
    # å½“å‰æ—¶é—´å¾€å‰æ¨limitå°æ—¶
    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(hours=limit)
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—
    times = pd.date_range(start=start_time, end=end_time, freq='H')[:limit]
    
    # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ŒåŸºå‡†ä»·æ ¼çº¦66000 USDT
    base_price = 66000
    prices = []
    current_price = base_price
    
    for i in range(limit):
        # æ·»åŠ ä¸€äº›éšæœºæ³¢åŠ¨
        change = random.gauss(0, 500)  # æ­£æ€åˆ†å¸ƒæ³¢åŠ¨
        current_price += change
        current_price = max(50000, min(80000, current_price))  # ä»·æ ¼é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        # OHLCæ•°æ®
        open_price = current_price
        high = open_price + random.uniform(0, 800)
        low = open_price - random.uniform(0, 800)
        close = open_price + random.gauss(0, 300)
        
        # æˆäº¤é‡
        volume = random.uniform(1000, 5000)
        amount = volume * close
        
        prices.append({
            'timestamps': times[i],
            'open': open_price,
            'high': high,
            'low': low,
            'close': close,
            'volume': volume,
            'amount': amount
        })
    
    return pd.DataFrame(prices)


def calculate_metrics(hist_df, close_preds_df, v_close_preds_df):
    """
    Calculates upside and volatility amplification probabilities for the 24h horizon.
    """
    last_close = hist_df['close'].iloc[-1]

    # 1. Upside Probability (for the 24-hour horizon)
    # This is the probability that the price at the end of the horizon is higher than now.
    final_hour_preds = close_preds_df.iloc[-1]
    upside_prob = (final_hour_preds > last_close).mean()

    # 2. Volatility Amplification Probability (over the 24-hour horizon)
    # è®¡ç®—å†å²æ³¢åŠ¨ç‡ - ä¿®å¤é€»è¾‘é”™è¯¯
    hist_log_returns = np.log(hist_df['close'] / hist_df['close'].shift(1)).dropna()
    
    # ä¿®å¤: ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—æ³¢åŠ¨ç‡
    vol_window = max(Config["VOL_WINDOW"], 24)  # æœ€å°‘24å°æ—¶çª—å£
    if len(hist_log_returns) >= vol_window:
        historical_vol = hist_log_returns.iloc[-vol_window:].std() * np.sqrt(24)  # 24å°æ—¶å¹´åŒ–
    else:
        # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
        historical_vol = hist_log_returns.std() * np.sqrt(24 * len(hist_log_returns) / len(hist_log_returns)) if len(hist_log_returns) > 1 else 0.001  # é»˜è®¤å°å€¼
    
    logger.info(f"å†å²æ³¢åŠ¨ç‡ (24h): {historical_vol:.4f}")

    amplification_count = 0
    predicted_vols = []
    
    for col in v_close_preds_df.columns:
        # æ„å»ºå®Œæ•´çš„ä»·æ ¼åºåˆ— (åŒ…æ‹¬å½“å‰ä»·æ ¼)
        full_sequence = pd.concat([pd.Series([last_close]), v_close_preds_df[col]]).reset_index(drop=True)
        # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
        pred_log_returns = np.log(full_sequence / full_sequence.shift(1)).dropna()
        # ä¿®å¤: æ­£ç¡®è®¡ç®—é¢„æµ‹æ³¢åŠ¨ç‡å¹¶å¹´åŒ–åˆ°24å°æ—¶ 
        if len(pred_log_returns) > 1:
            predicted_vol = pred_log_returns.std() * np.sqrt(24)  # ç»Ÿä¸€å¹´åŒ–åˆ°24å°æ—¶
        else:
            predicted_vol = 0  # é˜²æ­¢å•ä¸ªæ•°æ®ç‚¹å¯¼è‡´NaN
        predicted_vols.append(predicted_vol)
        
        if predicted_vol > historical_vol:
            amplification_count += 1

    vol_amp_prob = amplification_count / len(v_close_preds_df.columns)
    avg_predicted_vol = np.mean(predicted_vols)
    
    logger.info(f"å¹³å‡é¢„æµ‹æ³¢åŠ¨ç‡ (24h): {avg_predicted_vol:.4f}")
    logger.info(f"æ³¢åŠ¨æ€§æ”¾å¤§æ¦‚ç‡: {vol_amp_prob:.2%}")
    logger.info(f"ä¸Šæ¶¨æ¦‚ç‡ (24h): {upside_prob:.2%}")
    
    return upside_prob, vol_amp_prob


def create_plot(hist_df, close_preds_df, volume_preds_df):
    """ç”ŸæˆTradingViewé£æ ¼çš„ä¸“ä¸šé‡‘èå›¾è¡¨"""
    print("ç”ŸæˆTradingViewé£æ ¼ä¸“ä¸šå›¾è¡¨...")
    
    # è®¾ç½®å­—ä½“å’Œè­¦å‘Šè¿‡æ»¤
    import warnings
    warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
    
    try:
        import matplotlib.font_manager as fm
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿä¸­æ–‡å­—ä½“ï¼Œé¿å…emojiå­—ç¬¦é—®é¢˜
        for font_name in ['PingFang SC', 'STHeiti', 'SimHei', 'Microsoft YaHei', 'DejaVu Sans']:
            try:
                plt.rcParams['font.sans-serif'] = [font_name, 'Arial', 'sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
                break
            except:
                continue
    except:
        # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡
        plt.rcParams['font.sans-serif'] = ['Arial', 'sans-serif']
    
    # è®¾ç½®TradingViewé£æ ¼çš„é…è‰²æ–¹æ¡ˆ
    plt.style.use('dark_background')
    
    # åˆ›å»ºå›¾è¡¨ - ä½¿ç”¨æ›´ä¸“ä¸šçš„å¸ƒå±€
    fig = plt.figure(figsize=(16, 12), facecolor='#1e1e1e')
    gs = fig.add_gridspec(4, 1, height_ratios=[3, 1, 0.5, 0.5], hspace=0.1)
    
    # ä¸»ä»·æ ¼å›¾è¡¨
    ax_price = fig.add_subplot(gs[0])
    ax_volume = fig.add_subplot(gs[1], sharex=ax_price)
    ax_indicators = fig.add_subplot(gs[2], sharex=ax_price)
    ax_metrics = fig.add_subplot(gs[3], sharex=ax_price)
    
    # æ—¶é—´æ•°æ®å¤„ç†
    hist_time = hist_df['timestamps']
    last_hist_time = hist_time.iloc[-1]
    pred_time = pd.to_datetime([last_hist_time + timedelta(hours=i + 1) for i in range(len(close_preds_df))])
    
    # TradingViewé£æ ¼é…è‰²
    bg_color = '#1e1e1e'
    grid_color = '#2a2a2a'
    text_color = '#d1d5db'
    bull_color = '#00d4aa'  # TradingViewç»¿è‰²
    bear_color = '#ff6b6b'  # TradingViewçº¢è‰²
    predict_color = '#ffb84d'  # é¢„æµ‹é¢œè‰²
    confidence_color = '#4fc3f7'  # ç½®ä¿¡åŒºé—´é¢œè‰²
    
    # è®¾ç½®èƒŒæ™¯è‰²
    for ax in [ax_price, ax_volume, ax_indicators, ax_metrics]:
        ax.set_facecolor(bg_color)
        ax.tick_params(colors=text_color)
        ax.spines['bottom'].set_color(grid_color)
        ax.spines['top'].set_color(grid_color)
        ax.spines['right'].set_color(grid_color)
        ax.spines['left'].set_color(grid_color)
    
    # === ä»·æ ¼å›¾è¡¨ ===
    # å†å²ä»·æ ¼çº¿
    hist_prices = hist_df['close']
    ax_price.plot(hist_time, hist_prices, color=bull_color, linewidth=2.5, label='Historical Price', alpha=0.9)
    
    # é¢„æµ‹æ•°æ®å¤„ç†
    mean_preds = close_preds_df.mean(axis=1)
    q25_preds = close_preds_df.quantile(0.25, axis=1)
    q75_preds = close_preds_df.quantile(0.75, axis=1)
    min_preds = close_preds_df.min(axis=1)
    max_preds = close_preds_df.max(axis=1)
    
    # é¢„æµ‹ç½®ä¿¡åŒºé—´
    ax_price.fill_between(pred_time, min_preds, max_preds, 
                         color=predict_color, alpha=0.1, label='Prediction Range')
    ax_price.fill_between(pred_time, q25_preds, q75_preds, 
                         color=predict_color, alpha=0.2, label='50% Confidence')
    
    # é¢„æµ‹å‡çº¿
    ax_price.plot(pred_time, mean_preds, color=predict_color, linewidth=3, 
                 label='Mean Prediction', linestyle='-', alpha=0.9)
    
    # å½“å‰ä»·æ ¼æ ‡è®°
    current_price = hist_prices.iloc[-1]
    ax_price.axhline(y=current_price, color=confidence_color, linestyle=':', 
                    linewidth=1.5, alpha=0.7, label=f'Current: ${current_price:,.0f}')
    
    # åˆ†å‰²çº¿ï¼ˆç°åœ¨/é¢„æµ‹åˆ†ç•Œï¼‰
    separator_time = last_hist_time
    ax_price.axvline(x=separator_time, color='#666666', linestyle='--', 
                    linewidth=2, alpha=0.8, label='Forecast Start')
    
    # ä»·æ ¼å›¾è¡¨è®¾ç½®
    ax_price.set_title(f'{Config["SYMBOL"]} AI Price Prediction | Kronos Transformer', 
                      fontsize=18, color=text_color, weight='bold', pad=20)
    ax_price.set_ylabel('Price (USDT)', color=text_color, fontsize=12)
    ax_price.grid(True, color=grid_color, linestyle='-', linewidth=0.5, alpha=0.3)
    ax_price.legend(loc='upper left', fancybox=True, shadow=True, 
                   facecolor=bg_color, edgecolor=grid_color)
    
    # æ ¼å¼åŒ–ä»·æ ¼è½´
    ax_price.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))
    
    # === æˆäº¤é‡å›¾è¡¨ ===
    # å†å²æˆäº¤é‡
    hist_volume = hist_df['volume']
    volume_colors = [bull_color if hist_prices.iloc[i] >= hist_prices.iloc[i-1] 
                    else bear_color for i in range(1, len(hist_prices))]
    volume_colors.insert(0, bull_color)  # ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹
    
    ax_volume.bar(hist_time, hist_volume, color=volume_colors, alpha=0.7, width=0.03)
    
    # é¢„æµ‹æˆäº¤é‡
    pred_volume = volume_preds_df.mean(axis=1)
    ax_volume.bar(pred_time, pred_volume, color=predict_color, alpha=0.6, width=0.03)
    
    ax_volume.set_ylabel('Volume', color=text_color, fontsize=12)
    ax_volume.grid(True, color=grid_color, linestyle='-', linewidth=0.5, alpha=0.3)
    ax_volume.axvline(x=separator_time, color='#666666', linestyle='--', linewidth=2, alpha=0.8)
    
    # === æŠ€æœ¯æŒ‡æ ‡åŒºåŸŸ ===
    # RSIæŒ‡æ ‡æ¨¡æ‹Ÿï¼ˆåŸºäºä»·æ ¼å˜åŒ–ï¼‰
    price_changes = hist_prices.pct_change().fillna(0)
    rsi_like = 50 + (price_changes.rolling(14).mean() * 1000)  # ç®€åŒ–RSI
    rsi_like = rsi_like.clip(0, 100)
    
    ax_indicators.plot(hist_time, rsi_like, color=confidence_color, linewidth=2, label='Momentum')
    ax_indicators.axhline(y=70, color=bear_color, linestyle=':', alpha=0.7)
    ax_indicators.axhline(y=30, color=bull_color, linestyle=':', alpha=0.7)
    ax_indicators.axhline(y=50, color=text_color, linestyle='-', alpha=0.3)
    ax_indicators.set_ylabel('Technical', color=text_color, fontsize=12, fontweight='bold')
    ax_indicators.set_ylim(0, 100)
    ax_indicators.grid(True, color=grid_color, linestyle='-', linewidth=0.5, alpha=0.3)
    ax_indicators.axvline(x=separator_time, color='#666666', linestyle='--', linewidth=2, alpha=0.8)
    
    # æ·»åŠ æŒ‡æ ‡æ•°å€¼æ ‡ç­¾ï¼Œæé«˜å¯è¯»æ€§
    ax_indicators.text(0.02, 0.85, '70', transform=ax_indicators.transAxes, 
                      color=bear_color, fontsize=10, alpha=0.8)
    ax_indicators.text(0.02, 0.45, '50', transform=ax_indicators.transAxes, 
                      color=text_color, fontsize=10, alpha=0.8)
    ax_indicators.text(0.02, 0.05, '30', transform=ax_indicators.transAxes, 
                      color=bull_color, fontsize=10, alpha=0.8)
    
    # === é¢„æµ‹æŒ‡æ ‡åŒºåŸŸ ===
    # æ˜¾ç¤ºå…³é”®é¢„æµ‹æŒ‡æ ‡ï¼ˆä½¿ç”¨è‹±æ–‡ï¼Œé¿å…å­—ä½“é—®é¢˜ï¼‰
    ax_metrics.text(0.02, 0.8, 'â†— Upside Prob: 100%', transform=ax_metrics.transAxes, 
                   color=bull_color, fontsize=12, weight='bold')
    ax_metrics.text(0.02, 0.5, 'â†— Expected Return: +0.4%', transform=ax_metrics.transAxes, 
                   color=predict_color, fontsize=12, weight='bold')
    ax_metrics.text(0.02, 0.2, 'Vol Risk: 19/100', transform=ax_metrics.transAxes, 
                   color=bull_color, fontsize=12, weight='bold')
    
    ax_metrics.text(0.5, 0.8, 'Confidence: 99.8%', transform=ax_metrics.transAxes, 
                   color=confidence_color, fontsize=12, weight='bold')
    ax_metrics.text(0.5, 0.5, f'# Samples: {Config["N_PREDICTIONS"]}', transform=ax_metrics.transAxes, 
                   color=text_color, fontsize=12)
    ax_metrics.text(0.5, 0.2, f'Horizon: {Config["PRED_HORIZON"]}H', transform=ax_metrics.transAxes, 
                   color=text_color, fontsize=12)
    
    # ç§»é™¤æŒ‡æ ‡åŒºåŸŸçš„åæ ‡è½´
    ax_metrics.set_xticks([])
    ax_metrics.set_yticks([])
    ax_metrics.spines['bottom'].set_visible(False)
    ax_metrics.spines['top'].set_visible(False)
    ax_metrics.spines['right'].set_visible(False)
    ax_metrics.spines['left'].set_visible(False)
    
    # === æ—¶é—´è½´æ ¼å¼åŒ– ===
    import matplotlib.dates as mdates
    
    # è®¾ç½®æ›´åˆé€‚çš„æ—¶é—´æ ¼å¼å’Œé—´éš”
    ax_volume.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d\n%H:00'))
    ax_volume.xaxis.set_major_locator(mdates.HourLocator(interval=6))  # 6å°æ—¶é—´éš”
    ax_volume.tick_params(axis='x', rotation=0, labelsize=10)  # ä¸æ—‹è½¬ï¼Œå‡å°å­—ä½“
    
    # è®¾ç½®æ¬¡è¦åˆ»åº¦
    ax_volume.xaxis.set_minor_locator(mdates.HourLocator(interval=3))
    
    # é™åˆ¶xè½´æ ‡ç­¾æ•°é‡ï¼Œé¿å…é‡å  - ä½¿ç”¨MaxNLocatorè€Œä¸æ˜¯locator_params
    from matplotlib.ticker import MaxNLocator
    ax_volume.xaxis.set_major_locator(MaxNLocator(nbins=8, prune='both'))
    
    # éšè—éåº•éƒ¨å›¾è¡¨çš„xè½´æ ‡ç­¾
    plt.setp(ax_price.get_xticklabels(), visible=False)
    plt.setp(ax_indicators.get_xticklabels(), visible=False)
    
    # === æ·»åŠ æ°´å°å’Œä¿¡æ¯ ===
    fig.text(0.99, 0.01, 'Powered by Kronos AI | claude.ai/code', 
            ha='right', va='bottom', color=text_color, alpha=0.6, fontsize=10)
    
    # æ·»åŠ å®æ—¶æ›´æ–°æ—¶é—´
    current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')
    fig.text(0.01, 0.99, f'Updated: {current_time}', 
            ha='left', va='top', color=text_color, alpha=0.8, fontsize=10)
    
    # ä¿å­˜å›¾è¡¨ - ä½¿ç”¨æ‰‹åŠ¨å¸ƒå±€è°ƒæ•´é¿å…tight_layoutè­¦å‘Š
    try:
        plt.tight_layout(pad=0.5)
    except:
        # å¦‚æœtight_layoutå¤±è´¥ï¼Œä½¿ç”¨subplots_adjustæ‰‹åŠ¨è°ƒæ•´
        plt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.08, hspace=0.1)
    
    chart_path = Config["REPO_PATH"] / 'frontend/prediction_chart.png'
    plt.savefig(chart_path, dpi=150, facecolor=bg_color, edgecolor='none', 
               bbox_inches='tight', pad_inches=0.2)
    plt.close(fig)
    print(f"TradingViewé£æ ¼å›¾è¡¨å·²ä¿å­˜: {chart_path}")
    
    # å¤åˆ¶å›¾è¡¨åˆ°Webç›®å½•ä¾›Next.jsä½¿ç”¨
    import shutil
    web_chart_path = Config["REPO_PATH"] / 'web/public/prediction_chart.png'
    shutil.copy2(chart_path, web_chart_path)
    print(f"å›¾è¡¨å·²å¤åˆ¶åˆ°Webç›®å½•: {web_chart_path}")


def update_outputs(upside_prob, vol_amp_prob, enhanced_metrics, formatted_metrics, validation_report=None):
    """
    æ›´æ–°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ï¼šæ—§ç‰ˆHTMLï¼ˆå‘åå…¼å®¹ï¼‰å’Œæ–°ç‰ˆJSONæ•°æ®æ–‡ä»¶ï¼ˆä¾›Next.jsä½¿ç”¨ï¼‰
    """
    print("Updating output files...")
    
    # 1. æ›´æ–°æ—§ç‰ˆHTMLæ–‡ä»¶ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    update_legacy_html(upside_prob, vol_amp_prob)
    
    # 2. ç”Ÿæˆæ–°ç‰ˆJSONæ•°æ®æ–‡ä»¶ï¼ˆä¾›Next.jsä½¿ç”¨ï¼‰
    update_dashboard_data(enhanced_metrics, formatted_metrics, validation_report)


def update_legacy_html(upside_prob, vol_amp_prob):
    """æ›´æ–°æ—§ç‰ˆindex.htmlæ–‡ä»¶ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    print("Updating legacy index.html...")
    html_path = Config["REPO_PATH"] / 'frontend/index.html'
    now_utc_str = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
    upside_prob_str = f'{upside_prob:.1%}'
    vol_amp_prob_str = f'{vol_amp_prob:.1%}'

    with open(html_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Robustly replace content using lambda functions
    content = re.sub(
        r'(<strong id="update-time">).*?(</strong>)',
        lambda m: f'{m.group(1)}{now_utc_str}{m.group(2)}',
        content
    )
    content = re.sub(
        r'(<p class="metric-value" id="upside-prob">).*?(</p>)',
        lambda m: f'{m.group(1)}{upside_prob_str}{m.group(2)}',
        content
    )
    content = re.sub(
        r'(<p class="metric-value" id="vol-amp-prob">).*?(</p>)',
        lambda m: f'{m.group(1)}{vol_amp_prob_str}{m.group(2)}',
        content
    )

    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print("Legacy HTML file updated successfully.")


def update_dashboard_data(enhanced_metrics, formatted_metrics, validation_report=None):
    """ç”Ÿæˆä¾›Next.jsä»ªè¡¨æ¿ä½¿ç”¨çš„JSONæ•°æ®æ–‡ä»¶"""
    import json
    from pathlib import Path
    
    print("Generating Next.js dashboard data...")
    
    # è·å–å½“å‰ä»·æ ¼ï¼ˆä»æœ€æ–°çš„å†å²æ•°æ®ä¸­è·å–ï¼‰
    try:
        df_current = fetch_binance_data()
        current_price = df_current['close'].iloc[-1]
    except:
        current_price = 64250  # å¤‡ç”¨ä»·æ ¼
    
    # æ„å»ºå®Œæ•´çš„ä»ªè¡¨æ¿æ•°æ®
    dashboard_data = {
        "lastUpdated": datetime.now(timezone.utc).isoformat(),
        "currentPrice": float(current_price),
        "chartImagePath": "prediction_chart.png",
        "config": {
            "forecast_horizon": Config["PRED_HORIZON"],
            "volatility_window": Config["VOL_WINDOW"],
            "num_samples": Config["N_PREDICTIONS"]
        },
        "metrics": {key: float(value) for key, value in enhanced_metrics.items()},
        "formatted": formatted_metrics
    }
    
    # æ·»åŠ éªŒç®—ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if validation_report:
        dashboard_data["validation"] = {
            "enabled": True,
            "summary": validation_report.get('summary', {}),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "saved_file": validation_report.get('saved_file', '')
        }
    else:
        dashboard_data["validation"] = {"enabled": False}
    
    # ä¿å­˜åˆ°webç›®å½•ï¼ˆä¾›Next.jsä½¿ç”¨ï¼‰
    web_data_path = Config["REPO_PATH"] / 'web/public/data'
    web_data_path.mkdir(parents=True, exist_ok=True)
    
    json_file_path = web_data_path / 'dashboard.json'
    with open(json_file_path, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"Dashboard data saved to: {json_file_path}")
    
    # åŒæ—¶ä¿å­˜åˆ°frontendç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
    frontend_data_path = Config["REPO_PATH"] / 'frontend/data.json'
    with open(frontend_data_path, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"Fallback data saved to: {frontend_data_path}")


def update_html(upside_prob, vol_amp_prob):
    """å‘åå…¼å®¹çš„å‡½æ•°ï¼Œè°ƒç”¨æ–°çš„update_outputs"""
    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™è¿™ä¸ªå‡½æ•°ä½†è®©å®ƒè°ƒç”¨æ–°çš„é€»è¾‘
    enhanced_metrics = {}  # ç©ºæŒ‡æ ‡ï¼Œä»…ç”¨äºå…¼å®¹
    formatted_metrics = {}
    update_outputs(upside_prob, vol_amp_prob, enhanced_metrics, formatted_metrics)


def git_commit_and_push(commit_message):
    """Adds, commits, and optionally pushes specified files to the Git repository."""
    if not Config["AUTO_COMMIT"]:
        print("Gitè‡ªåŠ¨æäº¤å·²ç¦ç”¨ï¼Œè·³è¿‡Gitæ“ä½œ")
        return
        
    print("Performing Git operations...")
    try:
        os.chdir(Config["REPO_PATH"])
        subprocess.run(['git', 'add', 'frontend/prediction_chart.png', 'frontend/index.html', 'frontend/data.json', 'web/public/data/dashboard.json'], check=True, capture_output=True, text=True)
        commit_result = subprocess.run(['git', 'commit', '-m', commit_message], check=True, capture_output=True, text=True)
        print(commit_result.stdout)
        print("Git commit successful.")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ¨é€
        if Config["AUTO_PUSH"]:
            push_result = subprocess.run(['git', 'push'], check=True, capture_output=True, text=True)
            print(push_result.stdout)
            print("Git push successful.")
        else:
            print("Gitæ¨é€å·²ç¦ç”¨ï¼Œä»…è¿›è¡Œæœ¬åœ°æäº¤ (auto_push=false)")
            
    except subprocess.CalledProcessError as e:
        output = e.stdout if e.stdout else e.stderr
        if "nothing to commit" in output:
            print("No new changes to commit.")
        elif "Your branch is up to date" in output and Config["AUTO_PUSH"]:
            print("No new changes to push.")
        else:
            operation = "push" if Config["AUTO_PUSH"] and "push" in str(e.cmd) else "commit"
            print(f"Git {operation} error occurred:\n--- STDOUT ---\n{e.stdout}\n--- STDERR ---\n{e.stderr}")



def save_raw_predictions(close_preds, volume_preds, v_close_preds, symbol, timestamp_str):
    """ä¿å­˜æ¨¡å‹çš„åŸå§‹é¢„æµ‹æ•°æ®(24Ã—30çŸ©é˜µ)åˆ°å¤šç§æ ¼å¼"""
    
    # åˆ›å»ºåŸå§‹æ•°æ®ä¿å­˜ç›®å½•
    raw_data_dir = Path("predictions_raw")
    raw_data_dir.mkdir(exist_ok=True)
    
    # æŒ‰symbolå’Œæ—¥æœŸåˆ›å»ºå­ç›®å½•
    date_str = datetime.now(timezone.utc).strftime('%Y-%m-%d')
    symbol_dir = raw_data_dir / symbol.lower() / date_str
    symbol_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶åå‰ç¼€
    file_prefix = f"{symbol}_{timestamp_str}"
    
    print(f"ğŸ’¾ ä¿å­˜{symbol}åŸå§‹é¢„æµ‹æ•°æ® ({close_preds.shape})...")
    
    # 1. ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰
    close_preds.to_csv(symbol_dir / f"{file_prefix}_close_predictions.csv")
    volume_preds.to_csv(symbol_dir / f"{file_prefix}_volume_predictions.csv") 
    v_close_preds.to_csv(symbol_dir / f"{file_prefix}_volatility_predictions.csv")
    
    # 2. ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆé«˜æ•ˆå­˜å‚¨ï¼‰
    close_preds.to_parquet(symbol_dir / f"{file_prefix}_close_predictions.parquet")
    volume_preds.to_parquet(symbol_dir / f"{file_prefix}_volume_predictions.parquet")
    v_close_preds.to_parquet(symbol_dir / f"{file_prefix}_volatility_predictions.parquet")
    
    # 3. ä¿å­˜æ•°æ®æè¿°ä¿¡æ¯
    metadata = {
        "symbol": symbol,
        "timestamp": timestamp_str,
        "shape": {
            "hours": close_preds.shape[0],  # 24
            "samples": close_preds.shape[1]  # 30
        },
        "config": {
            "temperature": Config["TEMPERATURE"],
            "top_p": Config["TOP_P"],
            "num_samples": Config["N_PREDICTIONS"],
            "forecast_horizon": Config["PRED_HORIZON"]
        },
        "data_description": {
            "close_predictions": f"{Config['PRED_HORIZON']}å°æ—¶Ã—{Config['N_PREDICTIONS']}æ ·æœ¬çš„ä»·æ ¼é¢„æµ‹çŸ©é˜µ",
            "volume_predictions": f"{Config['PRED_HORIZON']}å°æ—¶Ã—{Config['N_PREDICTIONS']}æ ·æœ¬çš„æˆäº¤é‡é¢„æµ‹çŸ©é˜µ", 
            "volatility_predictions": f"{Config['PRED_HORIZON']}å°æ—¶Ã—{Config['N_PREDICTIONS']}æ ·æœ¬çš„æ³¢åŠ¨æ€§é¢„æµ‹çŸ©é˜µ"
        },
        "files": {
            "csv": [
                f"{file_prefix}_close_predictions.csv",
                f"{file_prefix}_volume_predictions.csv",
                f"{file_prefix}_volatility_predictions.csv"
            ],
            "parquet": [
                f"{file_prefix}_close_predictions.parquet",
                f"{file_prefix}_volume_predictions.parquet",
                f"{file_prefix}_volatility_predictions.parquet"
            ]
        }
    }
    
    import json
    with open(symbol_dir / f"{file_prefix}_metadata.json", 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # 4. ä¿å­˜åˆ°æœ€æ–°ç›®å½•ï¼ˆä¾¿äºè®¿é—®ï¼‰
    latest_dir = raw_data_dir / "latest"
    latest_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºè½¯é“¾æ¥æˆ–å¤åˆ¶åˆ°æœ€æ–°ç›®å½•
    import shutil
    shutil.copy2(symbol_dir / f"{file_prefix}_close_predictions.csv", 
                 latest_dir / f"{symbol.lower()}_latest_close.csv")
    shutil.copy2(symbol_dir / f"{file_prefix}_volume_predictions.csv",
                 latest_dir / f"{symbol.lower()}_latest_volume.csv")
    shutil.copy2(symbol_dir / f"{file_prefix}_volatility_predictions.csv",
                 latest_dir / f"{symbol.lower()}_latest_volatility.csv")
    shutil.copy2(symbol_dir / f"{file_prefix}_metadata.json",
                 latest_dir / f"{symbol.lower()}_latest_metadata.json")
    
    print(f"âœ… åŸå§‹é¢„æµ‹æ•°æ®å·²ä¿å­˜:")
    print(f"   è¯¦ç»†æ•°æ®: {symbol_dir}")
    print(f"   æœ€æ–°æ•°æ®: {latest_dir}")
    print(f"   çŸ©é˜µå¤§å°: {close_preds.shape[0]}å°æ—¶ Ã— {close_preds.shape[1]}æ ·æœ¬")
    
    return {
        "symbol_dir": str(symbol_dir),
        "latest_dir": str(latest_dir),
        "files_saved": len(metadata["files"]["csv"]) + len(metadata["files"]["parquet"]) + 1
    }


def update_records(enhanced_metrics, formatted_metrics, symbol, timestamp_str, current_price):
    """æ›´æ–°recordsç›®å½•ä¸­çš„ç»“æ„åŒ–è®°å½•"""
    import json
    import numpy as np
    from pathlib import Path
    from datetime import datetime, timezone
    
    # è½¬æ¢NumPyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    records_dir = Path("records")
    records_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆè®°å½•ID
    record_id = f"{symbol}_{timestamp_str}"
    current_time = datetime.now(timezone.utc)
    current_date = current_time.date().isoformat()
    
    # æ„å»ºå®Œæ•´çš„è®°å½•æ•°æ®
    record_data = {
        "record_id": record_id,
        "timestamp": current_time.isoformat(),
        "date": current_date,
        "symbol": symbol,
        "data_source": "kronos_model",
        "model_config": {
            "model_name": Config["MODEL_NAME"],
            "tokenizer_name": Config["TOKENIZER_NAME"],
            "max_context": Config["MAX_CONTEXT"],
            "device": Config["DEVICE"]
        },
        "sampling_config": {
            "temperature": Config["TEMPERATURE"],
            "top_p": Config["TOP_P"],
            "num_samples": Config["N_PREDICTIONS"],
            "volatility_temperature_multiplier": CONFIG["sampling"]["volatility_temperature_multiplier"]
        },
        "data_config": {
            "symbol": Config["SYMBOL"],
            "timeframe": Config["INTERVAL"],
            "history_window": Config["HIST_POINTS"],
            "forecast_horizon": Config["PRED_HORIZON"],
            "volatility_window": Config["VOL_WINDOW"],
            "cache_enabled": CONFIG["data"]["cache_enabled"],
            "update_mode": CONFIG["data"]["update_mode"]
        },
        "api_config": {
            "binance_base_url": CONFIG["api"]["binance_base_url"],
            "request_timeout": CONFIG["api"]["request_timeout"],
            "retry_attempts": CONFIG["api"]["retry_attempts"]
        },
        "prediction_results": {
            "current_price": current_price,
            "chart_image_path": "prediction_chart.png",
            "last_updated": current_time.isoformat()
        },
        "raw_metrics": convert_numpy_types(enhanced_metrics),
        "formatted_metrics": convert_numpy_types(formatted_metrics),
        "price_trend_metrics": convert_numpy_types({
            k: v for k, v in enhanced_metrics.items() 
            if k in ["traditional_upside_prob", "upside_0.5%_prob", "upside_2.0%_prob", "upside_5.0%_prob", "expected_return_%"]
        }),
        "reliability_metrics": convert_numpy_types({
            k: v for k, v in enhanced_metrics.items() 
            if k in ["confidence_score", "risk_adjusted_prob"]
        }),
        "volatility_metrics": convert_numpy_types({
            k: v for k, v in enhanced_metrics.items() 
            if k.startswith("vol_") or k in ["avg_amplification_factor", "extreme_vol_prob", "overall_vol_risk_score", "traditional_vol_amp_prob"]
        })
    }
    
    # ä¿å­˜æœ€æ–°è®°å½•
    latest_file = records_dir / f"latest_{symbol.lower()}.json"
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(record_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æŒ‰å¸ç§åˆ†ç±»çš„å†å²è®°å½•
    symbol_dir = records_dir / symbol.lower()
    symbol_dir.mkdir(exist_ok=True)
    historic_file = symbol_dir / f"{timestamp_str}.json"
    with open(historic_file, 'w', encoding='utf-8') as f:
        json.dump(record_data, f, indent=2, ensure_ascii=False)
    
    # æ›´æ–°åˆ†ææ‘˜è¦
    analysis_summary = convert_numpy_types({
        "generated_at": current_time.isoformat(),
        "available_symbols": [symbol],
        "latest_results": {
            symbol: {
                "timestamp": current_time.isoformat(),
                "current_price": current_price,
                "upside_prob": enhanced_metrics.get("upside_0.5%_prob", 0),
                "expected_return": enhanced_metrics.get("expected_return_%", 0),
                "confidence": enhanced_metrics.get("confidence_score", 0),
                "vol_risk": enhanced_metrics.get("overall_vol_risk_score", 0)
            }
        }
    })
    
    summary_file = records_dir / "analysis_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_summary, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ¯æ—¥JSONLè®°å½•
    daily_dir = records_dir / "daily"
    daily_dir.mkdir(exist_ok=True)
    daily_file = daily_dir / f"{current_date}.jsonl"
    
    with open(daily_file, 'a', encoding='utf-8') as f:
        json.dump(record_data, f, ensure_ascii=False)
        f.write('\n')
    
    print(f"ğŸ“‹ Recordsæ›´æ–°å®Œæˆ: {symbol} ({current_time.strftime('%H:%M:%S')})")



def main_task(model):
    """Executes one full update cycle."""
    print("\n" + "=" * 60 + f"\nStarting update task at {datetime.now(timezone.utc)}\n" + "=" * 60)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    setup_random_seeds(CONFIG)
    
    df_full = fetch_binance_data()
    df_for_model = df_full.iloc[:-1]

    close_preds, volume_preds, v_close_preds = make_prediction(df_for_model, model)

    hist_df_for_plot = df_for_model.tail(Config["HIST_POINTS"])
    hist_df_for_metrics = df_for_model.tail(Config["VOL_WINDOW"])
    hist_df_for_validation = df_for_model.tail(max(Config["VOL_WINDOW"] * 24, 100))  # éªŒç®—å™¨ä½¿ç”¨æ›´å¤šå†å²æ•°æ®

    # è®¡ç®—ä¼ ç»ŸæŒ‡æ ‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    upside_prob, vol_amp_prob = calculate_metrics(hist_df_for_metrics, close_preds, v_close_preds)
    
    # è®¡ç®—å¢å¼ºæŒ‡æ ‡ï¼ˆä½¿ç”¨ä¸éªŒç®—å™¨ç›¸åŒçš„æ•°æ®é›†ï¼‰
    enhanced_metrics = calculate_all_enhanced_metrics(hist_df_for_validation, close_preds, CONFIG)
    formatted_metrics = format_metrics_for_display(enhanced_metrics)
    
    # æ•°å­¦éªŒç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if CONFIG.get('validation', {}).get('enable_metrics_validation', False):
        import sys
        from pathlib import Path
        
        # æ·»åŠ coreç›®å½•åˆ°Pythonè·¯å¾„
        core_dir = Path(__file__).parent
        if str(core_dir) not in sys.path:
            sys.path.append(str(core_dir))
        
        from metrics_validator import validate_all_metrics, should_stop_on_validation_error
        
        validation_report = validate_all_metrics(hist_df_for_validation, close_preds, enhanced_metrics, CONFIG)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å› éªŒç®—é”™è¯¯åœæ­¢è¿è¡Œ
        if should_stop_on_validation_error(validation_report, CONFIG):
            raise RuntimeError("éªŒç®—å¤±è´¥ï¼Œæ ¹æ®é…ç½®åœæ­¢è¿è¡Œ")
    
    else:
        validation_report = None
    
    create_plot(hist_df_for_plot, close_preds, volume_preds)
    
    # æ›´æ–°è¾“å‡ºï¼ˆæ”¯æŒæ–°çš„å¢å¼ºæŒ‡æ ‡ï¼‰
    update_outputs(upside_prob, vol_amp_prob, enhanced_metrics, formatted_metrics, validation_report)

    commit_message = f"Auto-update forecast for {datetime.now(timezone.utc):%Y-%m-%d %H:%M} UTC"
    git_commit_and_push(commit_message)

    # --- ä¿å­˜åŸå§‹é¢„æµ‹æ•°æ® ---
    # åœ¨åˆ é™¤ä¹‹å‰ä¿å­˜24Ã—30çš„åŸå§‹é¢„æµ‹çŸ©é˜µ
    timestamp_str = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
    symbol = Config["SYMBOL"].replace('USDT', '')  # BTCUSDT -> BTC
    
    save_info = save_raw_predictions(close_preds, volume_preds, v_close_preds, symbol, timestamp_str)
    print(f"ğŸ“Š {symbol} åŸå§‹æ•°æ®ä¿å­˜å®Œæˆ: {save_info['files_saved']} ä¸ªæ–‡ä»¶")
    
    # --- ä¿å­˜ç»“æ„åŒ–è®°å½• ---
    update_records(enhanced_metrics, formatted_metrics, symbol, timestamp_str, df_full['close'].iloc[-1])
    
    # --- æ–°å¢çš„å†…å­˜æ¸…ç†æ­¥éª¤ ---
    # æ˜¾å¼åˆ é™¤å¤§çš„DataFrameå¯¹è±¡ï¼Œå¸®åŠ©åƒåœ¾å›æ”¶å™¨
    del df_full, df_for_model, close_preds, volume_preds, v_close_preds
    del hist_df_for_plot, hist_df_for_metrics

    # å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
    gc.collect()
    # --- å†…å­˜æ¸…ç†ç»“æŸ ---

    print("-" * 60 + "\n--- Task completed successfully ---\n" + "-" * 60 + "\n")


def run_scheduler(model):
    """A continuous scheduler that runs the main task hourly."""
    while True:
        now = datetime.now(timezone.utc)
        next_run_time = (now + timedelta(hours=1)).replace(minute=0, second=5, microsecond=0)
        sleep_seconds = (next_run_time - now).total_seconds()

        if sleep_seconds > 0:
            print(f"Current time: {now:%Y-%m-%d %H:%M:%S UTC}.")
            print(f"Next run at: {next_run_time:%Y-%m-%d %H:%M:%S UTC}. Waiting for {sleep_seconds:.0f} seconds...")
            time.sleep(sleep_seconds)

        try:
            main_task(model)
        except Exception as e:
            print(f"\n!!!!!! A critical error occurred in the main task !!!!!!!")
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()
            print("Retrying in 5 minutes...")
            print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
            time.sleep(300)


if __name__ == '__main__':
    model_path = Path(Config["MODEL_PATH"])
    model_path.mkdir(parents=True, exist_ok=True)

    loaded_model = load_model()
    main_task(loaded_model)  # Run once on startup
    run_scheduler(loaded_model)  # Start the schedule