#!/usr/bin/env python3
"""
Kronos Live Forecast - æ•°æ®ç®¡ç†å™¨
å®ç°æ•°æ®ç¼“å­˜ã€æ›´æ–°å’Œç®¡ç†åŠŸèƒ½
"""

import os
import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path
import logging

from binance.client import Client
try:
    from update_predictions import CONFIG, logger
except ImportError:
    # å¦‚æœç‹¬ç«‹è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    CONFIG = {
        "data": {
            "symbol": "BTCUSDT",
            "timeframe": "1h", 
            "history_window": 360,
            "volatility_window": 24
        }
    }
    import logging
    logger = logging.getLogger(__name__)


class DataManager:
    """æ•°æ®ç®¡ç†å™¨ï¼šè´Ÿè´£ç¼“å­˜ã€æ›´æ–°å’Œæä¾›å†å²æ•°æ®"""
    
    def __init__(self, data_dir="data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.cache_file = self.data_dir / "btc_cache.parquet"
        self.metadata_file = self.data_dir / "metadata.json"
        self.db_file = self.data_dir / "market_data.db"
        
        # é…ç½®å‚æ•°
        self.symbol = CONFIG["data"]["symbol"]
        self.timeframe = CONFIG["data"]["timeframe"]
        self.required_hours = CONFIG["data"]["history_window"] + CONFIG["data"]["volatility_window"]
        
        logger.info(f"æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.data_dir}")
    
    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        if not self.cache_file.exists():
            return {"status": "empty", "count": 0, "latest_time": None}
            
        try:
            df = pd.read_parquet(self.cache_file)
            return {
                "status": "available",
                "count": len(df),
                "latest_time": df['timestamps'].max(),
                "earliest_time": df['timestamps'].min(),
                "file_size": self.cache_file.stat().st_size
            }
        except Exception as e:
            logger.error(f"è¯»å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}
    
    def need_update(self, max_age_hours=1):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®"""
        cache_info = self.get_cache_info()
        
        if cache_info["status"] != "available":
            return True, "ç¼“å­˜ä¸å­˜åœ¨æˆ–æŸå"
            
        if cache_info["count"] < self.required_hours:
            return True, f"æ•°æ®ä¸è¶³ï¼Œéœ€è¦{self.required_hours}å°æ—¶ï¼Œå½“å‰{cache_info['count']}å°æ—¶"
            
        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
        latest_time = cache_info["latest_time"]
        if isinstance(latest_time, str):
            latest_time = pd.to_datetime(latest_time)
        
        # ç¡®ä¿æ—¶é—´æˆ³æœ‰æ—¶åŒºä¿¡æ¯
        if latest_time.tz is None:
            latest_time = latest_time.tz_localize('UTC')
        elif latest_time.tz != timezone.utc:
            latest_time = latest_time.tz_convert('UTC')
            
        hours_old = (datetime.now(timezone.utc) - latest_time).total_seconds() / 3600
        
        if hours_old > max_age_hours:
            return True, f"æ•°æ®è¿‡æœŸï¼Œå·²è¿‡æ—¶{hours_old:.1f}å°æ—¶"
            
        return False, "ç¼“å­˜æ•°æ®å……è¶³ä¸”æ–°é²œ"
    
    def fetch_fresh_data(self, limit=1000):
        """ä»äº¤æ˜“æ‰€è·å–æ–°æ•°æ®"""
        logger.info(f"ä»äº¤æ˜“æ‰€è·å–{limit}æ¡æœ€æ–°æ•°æ®...")
        
        # å°è¯•æ–¹æ³•1: CCXTç»Ÿä¸€äº¤æ˜“æ‰€API (ä¸»æ¨æ–¹æ¡ˆ)
        try:
            from ccxt_data_source import CCXTDataSource
            ccxt_source = CCXTDataSource()
            
            # è½¬æ¢symbolæ ¼å¼ (BTCUSDT -> BTC/USDT)
            symbol_ccxt = self.symbol.replace('USDT', '/USDT').replace('BTC', 'BTC')
            if not '/' in symbol_ccxt:
                symbol_ccxt = 'BTC/USDT'  # é»˜è®¤
                
            df = ccxt_source.fetch_ohlcv_data(
                symbol=symbol_ccxt, 
                timeframe=self.timeframe, 
                limit=limit
            )
            
            if df is not None:
                logger.info(f"âœ… CCXTè·å–æ•°æ®æˆåŠŸ: {len(df)}æ¡æ•°æ®")
                logger.info(f"æ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
                return df
            else:
                logger.warning("CCXTè¿”å›ç©ºæ•°æ®")
                
        except Exception as e1:
            logger.warning(f"CCXTæ–¹æ³•å¤±è´¥: {e1}")
        
        # å°è¯•æ–¹æ³•2: ä¼ ç»ŸBinance API  
        try:
            client = Client()
            klines = client.get_klines(
                symbol=self.symbol, 
                interval=self.timeframe, 
                limit=limit
            )
            logger.info("é€šè¿‡Binance APIè·å–æ•°æ®æˆåŠŸ")
            df = self._convert_klines_to_df(klines)
            logger.info(f"è·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
            return df
            
        except Exception as e2:
            logger.warning(f"Binance APIå¤±è´¥: {e2}")
        
        # å°è¯•æ–¹æ³•3: ç›´æ¥REST API
        try:
            import requests
            url = "https://api.binance.com/api/v3/klines"
            params = {
                'symbol': self.symbol,
                'interval': self.timeframe,
                'limit': limit
            }
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            klines = response.json()
            logger.info("é€šè¿‡REST APIè·å–æ•°æ®æˆåŠŸ")
            df = self._convert_klines_to_df(klines)
            logger.info(f"è·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
            return df
            
        except Exception as e3:
            logger.warning(f"REST APIä¹Ÿå¤±è´¥: {e3}")
        
        # å°è¯•æ–¹æ³•4: å¤–éƒ¨æ•°æ®æº
        logger.info("å°è¯•ä½¿ç”¨å¤–éƒ¨æ•°æ®æº...")
        try:
            from external_data_sources import MultiSourceDataManager
            external_manager = MultiSourceDataManager()
            days = min(42, limit // 24)  # æ ¹æ®limitä¼°ç®—å¤©æ•°
            df = external_manager.get_data_with_fallback(days=days)
            if df is not None:
                logger.info(f"å¤–éƒ¨æ•°æ®æºè·å–æˆåŠŸ: {len(df)}æ¡æ•°æ®")
                return df
        except Exception as e4:
            logger.error(f"å¤–éƒ¨æ•°æ®æºä¹Ÿå¤±è´¥: {e4}")
        
        logger.error("æ‰€æœ‰æ•°æ®è·å–æ–¹æ³•éƒ½å¤±è´¥")
        return None
    
    def _convert_klines_to_df(self, klines):
        """è½¬æ¢Kçº¿æ•°æ®ä¸ºDataFrame"""
        cols = ['open_time', 'open', 'high', 'low', 'close', 'volume', 
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']
        
        df = pd.DataFrame(klines, columns=cols)
        df = df[['open_time', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']]
        df.rename(columns={'quote_asset_volume': 'amount', 'open_time': 'timestamps'}, inplace=True)
        
        df['timestamps'] = pd.to_datetime(df['timestamps'], unit='ms', utc=True)
        for col in ['open', 'high', 'low', 'close', 'volume', 'amount']:
            df[col] = pd.to_numeric(df[col])
            
        return df.sort_values('timestamps').reset_index(drop=True)
    
    def update_cache(self, force_full_update=False):
        """æ›´æ–°æœ¬åœ°ç¼“å­˜"""
        cache_info = self.get_cache_info()
        
        if force_full_update or cache_info["status"] != "available":
            # å…¨é‡æ›´æ–°
            logger.info("æ‰§è¡Œå…¨é‡æ•°æ®æ›´æ–°...")
            fresh_df = self.fetch_fresh_data(limit=1000)
            if fresh_df is None:
                return False
                
            # ä¿å­˜åˆ°ç¼“å­˜
            fresh_df.to_parquet(self.cache_file)
            self._save_metadata({"last_update": datetime.now(timezone.utc).isoformat()})
            logger.info(f"å…¨é‡æ›´æ–°å®Œæˆï¼Œç¼“å­˜äº†{len(fresh_df)}æ¡æ•°æ®")
            
            # è‡ªåŠ¨å¯¼å‡ºCSV
            self._export_to_csv(fresh_df)
            
            return True
        
        else:
            # å¢é‡æ›´æ–°
            logger.info("æ‰§è¡Œå¢é‡æ•°æ®æ›´æ–°...")
            existing_df = pd.read_parquet(self.cache_file)
            
            # è·å–æœ€æ–°æ•°æ®
            fresh_df = self.fetch_fresh_data(limit=100)  # åªè·å–æœ€æ–°100æ¡
            if fresh_df is None:
                return False
            
            # åˆå¹¶æ•°æ®ï¼Œå»é‡
            combined_df = pd.concat([existing_df, fresh_df]).drop_duplicates(
                subset=['timestamps'], keep='last'
            ).sort_values('timestamps').reset_index(drop=True)
            
            # ä¿ç•™æœ€æ–°çš„æ•°æ®ï¼ˆé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
            if len(combined_df) > 2000:
                combined_df = combined_df.tail(2000)
            
            # ä¿å­˜æ›´æ–°çš„ç¼“å­˜
            combined_df.to_parquet(self.cache_file)
            self._save_metadata({"last_update": datetime.now(timezone.utc).isoformat()})
            logger.info(f"å¢é‡æ›´æ–°å®Œæˆï¼Œå½“å‰ç¼“å­˜{len(combined_df)}æ¡æ•°æ®")
            
            # è‡ªåŠ¨å¯¼å‡ºCSV
            self._export_to_csv(combined_df)
            
            return True
    
    def get_data(self, hours=None):
        """è·å–æŒ‡å®šå°æ—¶æ•°çš„å†å²æ•°æ®"""
        if hours is None:
            hours = self.required_hours
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        need_update, reason = self.need_update()
        if need_update:
            logger.info(f"ç¼“å­˜éœ€è¦æ›´æ–°: {reason}")
            success = self.update_cache()
            if not success:
                logger.warning("æ•°æ®æ›´æ–°å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ—§ç¼“å­˜...")
        
        # ä»ç¼“å­˜è¯»å–æ•°æ®
        if not self.cache_file.exists():
            logger.error("æ²¡æœ‰å¯ç”¨çš„ç¼“å­˜æ•°æ®")
            return None
            
        try:
            df = pd.read_parquet(self.cache_file)
            
            # è¿”å›æœ€æ–°çš„Nå°æ—¶æ•°æ®
            if len(df) >= hours:
                result_df = df.tail(hours).copy()
                logger.info(f"ä»ç¼“å­˜è·å–{len(result_df)}å°æ—¶æ•°æ®æˆåŠŸ")
                return result_df
            else:
                logger.warning(f"ç¼“å­˜æ•°æ®ä¸è¶³ï¼Œéœ€è¦{hours}å°æ—¶ï¼Œåªæœ‰{len(df)}å°æ—¶")
                return df
                
        except Exception as e:
            logger.error(f"è¯»å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _save_metadata(self, metadata):
        """ä¿å­˜å…ƒæ•°æ®"""
        with open(self.metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _export_to_csv(self, df):
        """å¯¼å‡ºæ•°æ®ä¸ºCSVæ ¼å¼"""
        try:
            sys.path.insert(0, str(Path(__file__).parent.parent / "tools"))
            from csv_exporter import CSVExporter
            
            exporter = CSVExporter(data_dir=self.data_dir)
            symbol = self.symbol
            timeframe = self.timeframe
            
            csv_path = exporter.export_dataframe(df, symbol, timeframe)
            if csv_path:
                logger.info(f"ğŸ“„ CSVæ•°æ®å·²å¯¼å‡º: {Path(csv_path).name}")
            
            # æ¸…ç†æ—§CSVæ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°3ä¸ª
            exporter.cleanup_old_csv_files(keep_latest=3)
            
        except Exception as e:
            logger.warning(f"CSVå¯¼å‡ºå¤±è´¥: {e}")
    
    def get_cache_status(self):
        """è·å–è¯¦ç»†çš„ç¼“å­˜çŠ¶æ€"""
        cache_info = self.get_cache_info()
        need_update, reason = self.need_update()
        
        status = {
            "cache_info": cache_info,
            "need_update": need_update,
            "update_reason": reason,
            "required_hours": self.required_hours
        }
        
        return status


# å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹
data_manager = DataManager()


def get_market_data():
    """ç»Ÿä¸€çš„æ•°æ®è·å–æ¥å£"""
    return data_manager.get_data()


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®ç®¡ç†å™¨
    logger.info("=== æ•°æ®ç®¡ç†å™¨æµ‹è¯• ===")
    
    status = data_manager.get_cache_status()
    print(f"ç¼“å­˜çŠ¶æ€: {status}")
    
    # è·å–æ•°æ®æµ‹è¯•
    df = data_manager.get_data(hours=50)
    if df is not None:
        print(f"è·å–æ•°æ®æˆåŠŸ: {len(df)}è¡Œ")
        print(f"æ—¶é—´èŒƒå›´: {df['timestamps'].min()} è‡³ {df['timestamps'].max()}")
        print(f"æœ€æ–°ä»·æ ¼: {df['close'].iloc[-1]:.2f}")
    else:
        print("è·å–æ•°æ®å¤±è´¥")